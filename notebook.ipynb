{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_einsum(name: str, equation: str, operands: List[np.ndarray]):\n",
    "    print(f\"{'=' * 10} {name} {'=' * 10}\")\n",
    "    print(f\"equation: {equation}\\n\")\n",
    "    \n",
    "    print(f\"Before {name}:\\n{operands}\\n\")\n",
    "\n",
    "    numpy_result = np.einsum(equation, *operands)\n",
    "    torch_result = torch.einsum(equation, *[torch.tensor(operand) for operand in operands])\n",
    "    tf_result = tf.einsum(equation, *[tf.constant(operand) for operand in operands])\n",
    "    \n",
    "    assert np.allclose(numpy_result, torch_result.numpy()), \"Numpy result is different from Torch result\"\n",
    "    assert np.allclose(numpy_result, tf_result.numpy()), \"Numpy result is different from Tensorflow result\"\n",
    "    \n",
    "    print(\"All results are same!!\\n\")\n",
    "    print(f\"After {name}:\\n{numpy_result}\\n\")\n",
    "    \n",
    "    return numpy_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transpose\n",
    "\n",
    "$$\\color{green}{B_{j, i}} \\color{black}{=} \\color{red}{A_{i,j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 2-D transpose ==========\n",
      "equation: ij->ji\n",
      "\n",
      "Before 2-D transpose:\n",
      "[array([[0.3328484 , 0.44398874],\n",
      "       [0.62238239, 0.56042012]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After 2-D transpose:\n",
      "[[0.3328484  0.62238239]\n",
      " [0.44398874 0.56042012]]\n",
      "\n",
      "========== 3-D transpose ==========\n",
      "equation: ijk->kji\n",
      "\n",
      "Before 3-D transpose:\n",
      "[array([[[0.00550469, 0.73214113],\n",
      "        [0.32663122, 0.56243587]],\n",
      "\n",
      "       [[0.36366162, 0.27567707],\n",
      "        [0.79475574, 0.55243245]]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After 3-D transpose:\n",
      "[[[0.00550469 0.36366162]\n",
      "  [0.32663122 0.79475574]]\n",
      "\n",
      " [[0.73214113 0.27567707]\n",
      "  [0.56243587 0.55243245]]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.00550469, 0.36366162],\n",
       "        [0.32663122, 0.79475574]],\n",
       "\n",
       "       [[0.73214113, 0.27567707],\n",
       "        [0.56243587, 0.55243245]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-D Matrix Transpose\n",
    "test_matrix = np.random.uniform(size=[2,2])\n",
    "check_einsum(\"2-D transpose\", \"ij->ji\", [test_matrix])\n",
    "\n",
    "# 3-D Matrix Transpose\n",
    "test_matrix = np.random.uniform(size=[2,2,2])\n",
    "check_einsum(\"3-D transpose\", \"ijk->kji\", [test_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sum\n",
    "\n",
    "$$\\color{green}{b} \\color{black}{= \\sum_{i}\\sum_{j}} \\color{red}{A_{i,j}} \\color{black}{=} \\color{red}{A_{i,j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 2-D Sum ==========\n",
      "equation: ij->\n",
      "\n",
      "Before 2-D Sum:\n",
      "[array([[0, 1],\n",
      "       [2, 3]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After 2-D Sum:\n",
      "6\n",
      "\n",
      "========== 3-D Sum ==========\n",
      "equation: ijk->\n",
      "\n",
      "Before 3-D Sum:\n",
      "[array([[[0, 1],\n",
      "        [2, 3]],\n",
      "\n",
      "       [[4, 5],\n",
      "        [6, 7]]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After 3-D Sum:\n",
      "28\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2-D Matrix Sum\n",
    "test_matrix = np.arange(4).reshape([2,2])\n",
    "check_einsum(\"2-D Sum\", \"ij->\", [test_matrix])\n",
    "\n",
    "# 3-D Matrix Sum\n",
    "test_matrix = np.arange(8).reshape([2,2,2])\n",
    "check_einsum(\"3-D Sum\", \"ijk->\", [test_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Column/Row Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Sum\n",
    "\n",
    "$$Column \\space sum: \\color{green}b_j \\color{black}{= \\sum_i} \\color{red}{A_{i,j}} \\color{black}{=} \\color{red}{A_{i,j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Sum\n",
    "\n",
    "$$Row \\space sum: \\color{green}b_i \\color{black}{= \\sum_j} \\color{red}{A_{i,j}} \\color{black}{=} \\color{red}{A_{i,j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 2-D Column Sum ==========\n",
      "equation: ij->j\n",
      "\n",
      "Before 2-D Column Sum:\n",
      "[array([[0, 1],\n",
      "       [2, 3]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After 2-D Column Sum:\n",
      "[2 4]\n",
      "\n",
      "========== 2-D Row Sum ==========\n",
      "equation: ij->i\n",
      "\n",
      "Before 2-D Row Sum:\n",
      "[array([[0, 1],\n",
      "       [2, 3]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After 2-D Row Sum:\n",
      "[1 5]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column Sum\n",
    "test_matrix = np.arange(4).reshape([2,2])\n",
    "check_einsum(\"2-D Column Sum\", \"ij->j\", [test_matrix])\n",
    "\n",
    "# Row Sum\n",
    "check_einsum(\"2-D Row Sum\", \"ij->i\", [test_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Matrix-Matrix(Vector) Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Vector Multiplication\n",
    "\n",
    "$$\\color{green}{c_i} \\color{black}{= \\sum_j} \\color{red}{A_{i,j}} \\color{blue}{b_j} \\color{black}{=} \\color{red}{A_{i,j}} \\color{blue}{b_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Matrix Multiplication\n",
    "\n",
    "$$\\color{green}{C_{i,j}} \\color{black}{=\\sum_k} \\color{red}{A_{i,k}} \\color{blue}{B_{k,j}} \\color{black}{=}  \\color{red}{A_{ik}} \\color{blue}{B_{kj}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Matrix-Vector Multiplication ==========\n",
      "equation: ij,j->i\n",
      "\n",
      "Before Matrix-Vector Multiplication:\n",
      "[array([[0, 1, 2],\n",
      "       [3, 4, 5]]), array([0, 1, 2])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Matrix-Vector Multiplication:\n",
      "[ 5 14]\n",
      "\n",
      "========== Matrix-Matrix Multiplication ==========\n",
      "equation: ik,kj->ij\n",
      "\n",
      "Before Matrix-Matrix Multiplication:\n",
      "[array([[0, 1, 2],\n",
      "       [3, 4, 5]]), array([[0, 3],\n",
      "       [1, 4],\n",
      "       [2, 5]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Matrix-Matrix Multiplication:\n",
      "[[ 5 14]\n",
      " [14 50]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5, 14],\n",
       "       [14, 50]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix = np.arange(6).reshape([2,3])\n",
    "test_vector = np.arange(3)\n",
    "check_einsum(\"Matrix-Vector Multiplication\", \"ij,j->i\", [test_matrix, test_vector])\n",
    "check_einsum(\"Matrix-Matrix Multiplication\", \"ik,kj->ij\", [test_matrix, test_matrix.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dot/Outer/Hadamard Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "\n",
    "$$Dot \\space product: \\color{green}{c} \\color{black}{= \\sum_i} \\color{red}{A_i} \\color{blue}{B_i} \\color{black}{=} \\color{red}{A_i} \\color{blue}{B_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Product\n",
    "\n",
    "$$Outer \\space product: \\color{green}{C_{i,j}} \\color{black}{=} \\color{red}{A_i} \\color{blue}{B_j} \\color{black}{=} \\color{red}{A_i} \\color{blue}{B_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadmard Product\n",
    "\n",
    "$$Hadamard \\space product: \\color{green}{C_{i,j}} \\color{black}{=} \\color{red}{A_{i,j}} \\color{blue}{B_{i,j}} \\color{black}{=} \\color{red}{A_{i,j}} \\color{blue}{B_{i,j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Dot product ==========\n",
      "equation: i,i->\n",
      "\n",
      "Before Dot product:\n",
      "[array([0, 1, 2]), array([0, 1, 2])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Dot product:\n",
      "5\n",
      "\n",
      "========== Outer product ==========\n",
      "equation: i,j->ij\n",
      "\n",
      "Before Outer product:\n",
      "[array([0, 1, 2]), array([0, 1, 2])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Outer product:\n",
      "[[0 0 0]\n",
      " [0 1 2]\n",
      " [0 2 4]]\n",
      "\n",
      "========== Hadamard product ==========\n",
      "equation: ij,ij->ij\n",
      "\n",
      "Before Hadamard product:\n",
      "[array([[0, 1, 2],\n",
      "       [3, 4, 5]]), array([[0, 1, 2],\n",
      "       [3, 4, 5]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Hadamard product:\n",
      "[[ 0  1  4]\n",
      " [ 9 16 25]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  4],\n",
       "       [ 9, 16, 25]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector_1 = np.arange(3)\n",
    "test_vector_2 = np.arange(3)\n",
    "\n",
    "# Dot Product\n",
    "check_einsum(\"Dot product\", \"i,i->\", [test_vector_1, test_vector_2])\n",
    "\n",
    "# Outer Product\n",
    "check_einsum(\"Outer product\", \"i,j->ij\", [test_vector_1, test_vector_2])\n",
    "\n",
    "test_matrix_1 = np.arange(6).reshape([2,3])\n",
    "test_matrix_2 = np.arange(6).reshape([2,3])\n",
    "\n",
    "# Hadamard Product\n",
    "check_einsum(\"Hadamard product\", \"ij,ij->ij\", [test_matrix_1, test_matrix_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Batch Matrix Multiplication\n",
    "\n",
    "$$\\color{green}{C_{ijl}} \\color{black}{=\\sum_k} \\color{red}{A_{ijk}} \\color{blue}{B_{ikl}} \\color{black}{=} \\color{red}{A_{ijk}} \\color{blue}{B_{ikl}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Batch Matrix Multiplication ==========\n",
      "equation: ijk,ikl->ijl\n",
      "\n",
      "Before Batch Matrix Multiplication:\n",
      "[array([[[0.81833742, 0.92045776]],\n",
      "\n",
      "       [[0.16013933, 0.09363739]]]), array([[[0.69330382, 0.85163496, 0.03238206],\n",
      "        [0.2413184 , 0.19979804, 0.73615102]],\n",
      "\n",
      "       [[0.97592846, 0.58599921, 0.95295187],\n",
      "        [0.35816464, 0.9281158 , 0.15611593]]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Batch Matrix Multiplication:\n",
      "[[[0.78947985 0.88083041 0.70409537]]\n",
      "\n",
      " [[0.18982213 0.18074786 0.16722336]]]\n",
      "\n",
      "original bmm result and einsum result are same!\n"
     ]
    }
   ],
   "source": [
    "i, j, k, l = 2, 1, 2, 3\n",
    "test_matrix_1 = np.random.uniform(size=(i,j,k))\n",
    "test_matrix_2 = np.random.uniform(size=(i,k,l))\n",
    "\n",
    "einsum_result = check_einsum(\"Batch Matrix Multiplication\", \"ijk,ikl->ijl\", [test_matrix_1, test_matrix_2])\n",
    "\n",
    "# coparision between einsum and original bmm\n",
    "\n",
    "original_bmm = torch.bmm(torch.tensor(test_matrix_1), torch.tensor(test_matrix_2)).numpy()\n",
    "assert np.all(original_bmm == einsum_result)\n",
    "print(\"original bmm result and einsum result are same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Bilinear Transformation\n",
    "\n",
    "$$\\color{green}{D_{ij}} \\color{black}{=\\sum_k\\sum_l} \\color{red}{A_{ik}} \\color{purple}{X_{jkl}} \\color{blue}{B_{il}} \\color{black}{=} \\color{red}{A_{ik}} \\color{purple}{X_{jkl}} \\color{blue}{B_{il}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Bilinear Transformation ==========\n",
      "equation: ik,jkl,il->ij\n",
      "\n",
      "Before Bilinear Transformation:\n",
      "[array([[0.21409295, 0.04719953],\n",
      "       [0.10524751, 0.09642733]]), array([[[0.88813922, 0.13008836],\n",
      "        [0.23644985, 0.15154198]],\n",
      "\n",
      "       [[0.90352873, 0.22726196],\n",
      "        [0.92723979, 0.90313901]],\n",
      "\n",
      "       [[0.79024212, 0.0382283 ],\n",
      "        [0.91830727, 0.84841075]]]), array([[0.0901718 , 0.80982622],\n",
      "       [0.9800735 , 0.11963888]])]\n",
      "\n",
      "All results are same!!\n",
      "\n",
      "After Bilinear Transformation:\n",
      "[[0.04649893 0.09531245 0.05822122]\n",
      " [0.11734401 0.19410949 0.17856814]]\n",
      "\n",
      "original bilinear result and einsum result are same!\n"
     ]
    }
   ],
   "source": [
    "i,j,k,l = 2,3,2,2\n",
    "test_matrix_1 = np.random.uniform(size=(i,k))\n",
    "test_matrix_2 = np.random.uniform(size=(i,l))\n",
    "X = np.random.uniform(size=(j,k,l))\n",
    "\n",
    "einsum_result = check_einsum(\"Bilinear Transformation\", \"ik,jkl,il->ij\", [test_matrix_1, X, test_matrix_2])\n",
    "\n",
    "original_bilinear = torch.nn.functional.bilinear(torch.tensor(test_matrix_1), \n",
    "                                                 torch.tensor(test_matrix_2), \n",
    "                                                 torch.tensor(X)).numpy()\n",
    "assert np.allclose(original_bilinear, einsum_result)\n",
    "print(\"original bilinear result and einsum result are same!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention(Advanced) \n",
    "\n",
    "[Attention is All you Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "$$MultiHeadAttention(K,Q,V) = concat(head_1, head_2, head_3, ...)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One head Attention\n",
    "\n",
    "$$head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^{\\top}}{\\sqrt{d_k}})V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def multihead_attention(hidden_states, W_Q, W_K, W_V, num_head):\n",
    "    batch_size, sequence_length, hidden_size = hidden_states.shape\n",
    "    assert hidden_size % num_head == 0\n",
    "    head_hidden_size = hidden_size // num_head\n",
    "\n",
    "    Q = np.einsum(\"ijk,kl->ijl\", hidden_states, W_Q)   # [batch_size, sequence_length, hidden_size]\n",
    "    K = np.einsum(\"ijk,kl->ijl\", hidden_states, W_K)   # [batch_size, sequence_length, hidden_size]\n",
    "    V = np.einsum(\"ijk,kl->ijl\", hidden_states, W_V)   # [batch_size, sequence_length, hidden_size]\n",
    "    print(f\"Q shape: {Q.shape} K shape: {K.shape} V shape: {V.shape}\")\n",
    "\n",
    "    Q = np.reshape(Q, [batch_size, sequence_length, num_head, head_hidden_size]) # [batch_size, sequence_length, num_haed, head_hidden_size]\n",
    "    K = np.reshape(K, [batch_size, sequence_length, num_head, head_hidden_size]) # [batch_size, sequence_length, num_haed, head_hidden_size]\n",
    "    V = np.reshape(V, [batch_size, sequence_length, num_head, head_hidden_size]) # [batch_size, sequence_length, num_haed, head_hidden_size]\n",
    "    print(f\"Q shape: {Q.shape} K shape: {K.shape} V shape: {V.shape}\")\n",
    "\n",
    "    Q = np.einsum(\"ijkl->ikjl\", Q)  # [batch_size, num_haed, sequence_length, head_hidden_size]\n",
    "    K = np.einsum(\"ijkl->ikjl\", K)  # [batch_size, num_haed, sequence_length, head_hidden_size]\n",
    "    V = np.einsum(\"ijkl->ikjl\", V)  # [batch_size, num_haed, sequence_length, head_hidden_size]\n",
    "    print(f\"Q shape: {Q.shape} K shape: {K.shape} V shape: {V.shape}\")\n",
    "\n",
    "    attention_score = np.einsum(\"ijkl,ijml->ijkm\", Q, K)/np.sqrt(hidden_size)  # [batch_size, num_haed, sequence_length, sequence_length]\n",
    "    attention_score = softmax(attention_score, axis=3)   # [batch_size, num_haed, sequence_length, sequence_length]\n",
    "    print(f\"Attention score shape: {attention_score.shape}\")\n",
    "    \n",
    "    attention_result = np.einsum(\"ijkl,ijkm->iljm\", attention_score, V)   # [batch_size, sequence_length, num_head, head_hidden_size]\n",
    "    attention_result = np.reshape(attention_result, [batch_size, sequence_length, hidden_size])  # [batch_size, sequence_length, hidden_size]\n",
    "    print(f\"Attention result shape: {attention_result.shape}\")\n",
    "    \n",
    "    return attention_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: (2, 10, 16) K shape: (2, 10, 16) V shape: (2, 10, 16)\n",
      "Q shape: (2, 10, 8, 2) K shape: (2, 10, 8, 2) V shape: (2, 10, 8, 2)\n",
      "Q shape: (2, 8, 10, 2) K shape: (2, 8, 10, 2) V shape: (2, 8, 10, 2)\n",
      "Attention score shape: (2, 8, 10, 10)\n",
      "Attention result shape: (2, 10, 16)\n"
     ]
    }
   ],
   "source": [
    "batch_size, sequence_length, hidden_size, num_head = 2, 10, 16, 8\n",
    "hidden_states = np.random.uniform(size=(batch_size, sequence_length, hidden_size))\n",
    "\n",
    "W_K = np.random.uniform(size=(hidden_size, hidden_size))\n",
    "W_Q = np.random.uniform(size=(hidden_size, hidden_size))\n",
    "W_V = np.random.uniform(size=(hidden_size, hidden_size))\n",
    "\n",
    "result = multihead_attention(hidden_states, W_Q, W_K, W_V, num_head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
